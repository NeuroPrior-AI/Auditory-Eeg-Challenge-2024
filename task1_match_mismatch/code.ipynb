{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\Auditory-Eeg-Challenge-2024\\task1_match_mismatch\\code.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Auditory-Eeg-Challenge-2024/task1_match_mismatch/code.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Auditory-Eeg-Challenge-2024/task1_match_mismatch/code.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/Auditory-Eeg-Challenge-2024/task1_match_mismatch/code.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Auditory-Eeg-Challenge-2024/task1_match_mismatch/code.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms, datasets, models\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/Auditory-Eeg-Challenge-2024/task1_match_mismatch/code.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m BCEWithLogitsLoss\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn.modules import flatten\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the contrastive loss with cosine similarity\n",
    "class ContrastiveCosineLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(ContrastiveCosineLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Cosine similarity\n",
    "        cos_sim = self.cosine_similarity(output1, output2)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = (1 - label) * 0.5 * cos_sim**2 + \\\n",
    "               label * 0.5 * (torch.relu(self.margin - cos_sim) ** 2)\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim, label_range=(0, 2)):\n",
    "        self.num_samples = num_samples\n",
    "        self.input_dim = input_dim\n",
    "        self.label_range = label_range\n",
    "        self.data = torch.randn(num_samples, input_dim)\n",
    "        self.labels = torch.randint(*label_range, (num_samples,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For simplicity, returning the same random vector for x1 and x2. \n",
    "        # In practice, you'd probably want pairs of samples (e.g., anchor and positive/negative).\n",
    "        x1 = self.data[idx]\n",
    "        x2 = self.data[(idx+1) % self.num_samples]  # Just for variety; wraps around at the end.\n",
    "        label = (self.labels[idx] == self.labels[(idx+1) % self.num_samples]).float()  # Label 1 if same, 0 if different\n",
    "        return x1, x2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder_1, encoder_2, criterion, optimizer, train_set, test_set, batch_size, num_epochs=10):\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Note encoder 1 is frezzing\n",
    "    encoder_1.eval()\n",
    "    for param in encoder_1.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Loop\n",
    "        encoder_2.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (x1, x2, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            embedding1 = encoder_1(x1)\n",
    "            embedding2 = encoder_2(x2)\n",
    "\n",
    "            loss = criterion(embedding1, embedding2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Testing Loop\n",
    "        encoder_2.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, labels in test_loader:\n",
    "                embedding1 = encoder_1(x1)\n",
    "                embedding2 = encoder_2(x2)\n",
    "\n",
    "                loss = criterion(embedding1, embedding2, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 512\n",
    "dataset = RandomDataset(num_samples=1000, input_dim=input_dim)\n",
    "\n",
    "encoder_1 = Encoder(input_dim, 1024)    # frozen pre-trained model\n",
    "encoder_2 = Encoder(input_dim, 1024)\n",
    "\n",
    "criterion = ContrastiveCosineLoss()\n",
    "optimizer = optim.Adam(encoder_2.parameters(), lr=0.001)\n",
    "\n",
    "train_length = int(0.8 * len(dataset))\n",
    "test_length = len(dataset) - train_length\n",
    "train_set, test_set = random_split(dataset, [train_length, test_length])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "train(encoder_1, encoder_2, criterion, optimizer, train_set, test_set, batch_size, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
